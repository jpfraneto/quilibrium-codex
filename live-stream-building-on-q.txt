if you've been a long time watcher, you'll definitely remember me going on long rants about that. It's so dumb to call it a server because it's usually a collection of microservices, so calling it a server is dumb. Then we have this notion of a collection of... What's the word I'm looking for? Direct messages, so DMs. And so the idea here is that we're going to kind of separate these two things out a little bit. And the reason for that is going to become very apparent as I iterate through the design choices that we're going to be making along the way. And then another important factor is going to be having file storage. You know, the irony here is that, you know, my goal here is to kill Amazon and I'm going to just blatantly shill an Amazon service here because it is very easy to use. And it is one of the first things that we want to target as a company as a API-compatible service. So it's more a testament to how useful the service is and less of a testament to Amazon being Amazon because actually, Qink doesn't even use S3. We don't use any Amazon web services. We use CloudFlare R2 for our CDN-related stuff. And so this talks about both file hosting and emojis, basically. And so the last thing that we need to get into is a notion of permissioning. And the permissioning, you'll note, actually relates to servers and guilds. And so we're not going to, you know, these aren't going to crop up in the context of DMs, but there is going to be something that we do care about in the context of DMs that is a little bit permissions-bound. And so then we're going to have a user repository, basically. User, whoops. This is also a classic Cassie-ism that people might remember is I can't talk and type. I'm really bad at it. I always like stumble over and mistype things. Okay. So now we have this collection of five different services and we need to iterate on what those structures do and how that relates. Now, normally when I'm doing a actual system design interview, I shotgun through all this super fast, but that's mostly because of time constraints. And I'd like to actually iterate through this in a way that helps people understand why people do these things. So from the authentication layer, a common composition that people use nowadays is that they will have this sort of idea of separating authentication from authorization. And the authorization will basically dictate what resources do you have access to and so on. We're not going to dive into the deep, deep details of that because there's loads of videos about that that people can find. But one thing that I do want to call out as a consequence of that type of separation in architecture is something that's useful regardless of whether or not you're using it for authorization as much as you are authentication, which is this kind of request response pattern that emerges where you will perform some sort of login with the actual authentication server. And then when you do that authentication, you will end up getting probably a cookie. Sometimes you'll get something a little bit more unique, but you'll get some sort of authentication attribute that stays relatively sticky to the auth services. And I'm going to be really specific there because all of these servers live likely on different subdomains, or at the very least internally, they're definitely living in separate domains and maybe exposed through a single load balancer that lumps it all together and under one API dot whatever. But at the end of the day, the auth server that you're authenticating against the cookie will only be good for that one. And so you won't want to create some gross shamongle of spreading a cookie across multiple servers and trying to manage that. So instead what you'll do is you will return back to the user a JWT or JSON web token. In the context of OpenID Connect and all those, they ultimately lead to this state where you do have some sort of token flow. I'm not going to, again, not going to deep dive into OIDC in this, but just talking about the end states here. And so you get this JSON web token and that JSON web token essentially is constructed with three parts. It's essentially like the header part, the intermediary set of claims, and then finally a signature. And what's important about that signature is that the signature, if you're familiar with public key cryptography, this will be a no brainer. That signature uses a private key that only the auth server has. And then all of these other servers have a public key that can essentially validate that that signature was in fact originating from the auth server. That actually gives you a great way to disconnect your auth server and not have to have a whole bunch of mother may I style API calls against the auth server, which is just a huge waste of time. This allows you to validate implicitly. And so once you have this flow incorporated, you end up with a locked in user session and then they can start requesting resources from all these other microservices. And so that takes care of the authentication flow in terms of interactions, but how does that actually look on the inside? And so in terms of designing and architecting that server, you could take the user route and say, oh, just use AWS Cognito or use Auth0 or use whatever kind of OIDC integration out there, lots of different options. But if you were wanting to work on this from a from scratch philosophy in the, and here's where I'm gonna do a little bit of interesting parallels. From the web two side, you would do something along the lines of either the authentication prebuilt service like Auth0 or you would have something that has a collection of user structures, basically, like you could have a document DB, you could have a simple key value store, you could have whatever it is that is necessary to contain that user information that you find to be the most useful. There's a lot of options and I'm not gonna iterate over all of them or their advantages or disadvantages, because honestly, the moment I do that, the tech changes, it doesn't matter. Because back in the day, people used to say things that were kind of almost memish, like MongoDB is web scale, which was obviously never true. But the idea was that no SQL style databases had performance advantages when it came to how data was partitioned and spread out in a way that made it very easy to treat it as one giant data store without having to think about, okay, which partition is this on and how do I manually partition this out? Nowadays, most RDBMSs let you do that on the fly and so those types of efficiency gains that you had to think about in the past are relatively gone. So I'm not gonna dive into the specifics of choice here, I'm just going to simply say that we have a data store that is sufficiently parallelizable that we can handle the large cluster of users. So I'm just going to blanket KV store. DynamoDB, Redis, whatever you want to use. I'm like Redis with obviously durability, but it doesn't matter. So we have our KV store and that is sufficiently parallelizable out to the point that we can easily handle millions of users in a single group or in terms of having millions of groups and millions of users and millions of users in each of those groups, potentially billions of users, it doesn't matter. As long as we have a sufficient number of KV stores that are sufficiently parallel and a sufficient number of authentication servers that expose it, then we are okay. And so when we actually leverage this KV store, we have essentially a load balancer. Oftentimes you will use like a ALB or NLB in terms of AWS parlance. This is kind of also one of the gripes that I have with system design interviews is that oftentimes it feels like you're just hawking a bunch of discrete microservices and like prebuilt Amazon services and it's just like a giant ad for AWS. But at the end of the day, that is kind of like the notion of what's happening here is that you are using tried and true services that have been battle tested. And so from this load balancer perspective, you have like maybe a collection of Docker images with your API container. Let me, oh Lord. Here's another classic moment. I'm fighting Excalidraw and the spacing of components. This is an eternal battle I still have not yet mastered. So apologies. Okay, so now we have our Docker images that are basically being sent out as a collection of endpoints for our API. Essentially, they are all in parallel, the same thing. We'll use a load balancer. That load balancer can leverage a sticky sessions strategy so that way we can keep the cookie that's being used to authenticate the user in memory. And so we don't have to worry so much about the session going invalid. We could also take it one step further and use a cookie that is built off of signed data and of itself. It's really just a series of choices that you can make at the implementation level. From a system design level, we just have a bunch of API servers, that's it. And so our auth server will be exposed through this load balancer. And then we will make a bunch of requests on the KV store in order to return that data back and then ultimately produce this JWT. Cool, basic web two approach. Now let's talk about the web three approach because this is one of the things I find quite remarkable about web three and I love it so much is that, no, you just have a wallet. There is no authentication server. You might have this notion of specialized contracts that relate to your particular wallet address but at the end of the day, and I'm going to use a little bit of differentiating terminology here because I am talking about like the classical web three experience. When you're on Ethereum or you're on Solana, you're dealing with wallets. When you're on Quilibrium, you're not actually having to deal with wallets. You're just using private keys appropriately but all of that is essentially taken care of for you with the use of pass keys. So instead of having to have a separate wallet application, you actually have a pass key and those pass keys are domain segregated which is something wallets in any other ecosystem just can't do. And that's honestly one of the reasons why so many people get wrecked. Oh Lord. All right, so from the perspective of how we think about these services, you're really just dealing with a public key and that public key is sufficient. Now you might want to have, as I said, info about the user, but that's a different topic. So we actually obviate a lot of the authentication problem by leaving it to the user and the management of their keys, whether through wallets or pass keys. So now we get to the concept of servers and guilds. This is where designs get really interesting and we actually take a lot of influence from that for the entire architecture of Quilibrium. And so here's what I'm going to shotgun through real fast. When it comes to how Discord works and how they store their data, across almost all of their services, they actually store their data in the form of a NoSQL database type called a wide column database. And wide column databases are generally, like generally better to scale out in terms of breadth. There may be some downsides in terms of how efficient they are for reads. So there's like certain trade-offs that you'll run into and there's lots of ways you can work around those trade-offs. The other thing is that relational components to it work very differently compared to like classical RDBMSs. But the reason why I'm bringing this up in particular is that relational databases do have an intrinsic scalability limit, even in the modern style. Whereas the architecture of how these wide column stores work is they work very explicitly off of this notion of a cluster first. Now, when we talk about Apache Cassandra, which no relation, but the database has this bad tendency to run into some interesting artifacts based on the history of how it was built. It's a little bit slower. It's a little bit more convoluted in keeping a cluster alive. It's just, there's lots of little tricky bits to it that make administrating a Apache Cassandra cluster kind of like a full-time job in and of itself. And that's been a long running meme amongst the SRE community out there. So I'm sure there's probably someone in here was like finally someone who understands. But alternatively, and what Discord took as a move was instead to use something that was CQL compatible, but is not actually Apache Cassandra. So they moved to a SilaDB. And SilaDB is fantastic because of the way that it's designed. The original authors of SilaDB took this approach that basically considers itself a shared nothing approach. They also were originally trying to build a operating system. So they definitely started from an interesting point and ended up in database land. But a lot of those learnings actually apply on the database level. And in fact, they are so tightly coupled to actual operating system work that SilaDB uses first off C star, which is a library that gets really close to the grain. It basically forces you to adopt this almost actor like model for the purpose of facilitating a philosophy called one thread per core. And one thread per core is this notion that the most efficient thing you can do for a processor is limit the amount of active threads that are being consumed, especially OS threads. But at the end of the day, the less that a processor needs to context switch, the faster your overall application will be. And so they did this thing called a thread per core. And the idea was that you will get from this, if you architect it correctly, a shared nothing outcome. And shared nothing basically means that you will have an application where all the individual cluster components, like all the shards of what you're building up, do not actually share any resources with one another. They might be in sync. Like there might be some synchronization that they have to do interoperably, but they themselves do not share any data in terms of actual resources. So they don't share RAM. They don't share cache on the CPU. They don't share a file attached store or a network attached storage for actually storing the data, even if there's redundancies involved. And so that type of approach allows for a way, way faster architecture that enables scalability that ultimately made SiloDB so fricking fast that Discord actually measures their message submission times in nanoseconds instead of milliseconds. And that's crazy. It's impressive. And that's the type of brilliant work of the SiloDB engineers when they created this entire architecture. And so a long walk for a short drink of water here is that we are going to have, of course, the corresponding load balancer that exposes things. And we're going to have a fleet of Docker images for our APIs, but our actual store here will definitely not be a KV store. We're going to use a wide column store that has this notion of shared nothing. And if we really wanna do this right and really wanna make this like super, super scalable, we would actually have the number of active clusters that we're maintaining, gearing up to some redundant level of APIs that are being called out to it. And the way that we would achieve that is to do some sort of partitioning scheme that makes sense for the actual data flow. And the reason I haven't dived into the data flow component of this system design process is because that too is going to be an interesting topic that relates to the web two, web three divide, but also into the context of how we're going to specifically architect things to make things work even better on cube. So right here, we're going to do this notion of 70B shard. And then from the DMs level, we'll basically have the same kind of concept. And there's actually going to be a lot of mirrored data structures that happen here. So when we talk about this in the web three context, we need to talk about how server data gets stored. So if you're familiar with developing for Ethereum, obviously you've got this global computer. You think about things in terms of, you have a smart contract that manages some collection data, and then that smart contract is storing it effectively in a Merkle tree route that is related to the account of the smart contract, divvied out in relation to the user that is making that particular request typically. That doesn't scale very well. So that was actually one of the things that we ran into very early on with equilibrium. Let's, let me see if I can actually find this because I think I took a screenshot of it a super long time ago. And I think that I have a slide somewhere where I talked about that. Oh, that makes me sad. I can't find it. Darn it. Okay. So yeah, back in the day, I actually like diagrammed this out and talked about the scalability of networks like Ethereum and how like, if you want to have even a single server, like if you want to architect your application as a smart contract such that a single server could sufficiently be represented as a single smart contract, you run into a little bit of a scaling issue. And so a lot of people in the Web3 context will actually do a lot of stuff like this and leave only basically authentication as the decentralized component. You see that with Farcaster, for example. Farcaster has a sufficiently decentralized approach with the hubs that cause data replication to occur out. But that data replication is completely irrespective of the Ethereum mainnet, or in the case of Farcaster, the Optimism mainnet. Because at the end of the day, you can't put all that content on a EVM-based network. Even if you had something like, what is that new one that's coming out? I really want to avoid like saying anything that could be construed as like me shit-talking a particular chain, because that's not the perspective I'm coming from. They are solving a different problem than what I'm trying to solve. It's just that if you're trying to use that to solve this problem, you will fail. So there's one out there that's now claiming that their mainnet will have like 100,000 TPS or something like that. Even then, you still run into a problem because if you're wanting to handle 400,000 messages a second, you've already failed. So when we're talking about how to construct this in the Web3 context, under classical Web3, you don't. You just don't. There is a social network that is trying to do that, trying to do most of the stuff on-chain. I'm not going to name their name because that would be rude. But they have been slowly but surely moving away from doing everything on-chain because they have found that that just isn't working. Plus it also costs money. So how do we think about it in terms of the equilibrium context? And so interestingly enough, from the equilibrium context, we do this same kind of notion of shard bases. So when you think about how data lands on the network, we actually have this like very massive grid and I'm not going to draw it all out, but we have this very massive grid of 256 global commitment shards. And then from there, let me see if I can actually sketch this out super well. Oh, that's neat. I didn't know I could do that, cool. So we have like 256 of these and these are the global commitments that are tracked. It's about 19 kilobytes of data. And from those, you have a collection of active core shards. Oh, hello, PicRuflow, is that right? I hope I said that right. And so from here, we have those 256 and then we have an additional 65,537. Oops, here we go. That will basically be in a massive bipartite graph. Like I said, I'm not going to actually diagram all that out because we would literally be spending, I would say on the order of at least a month trying to draw that out by hand. Just a wild guess, but it would take too damn long and I'm not going to do that. So we have this bipartite graph that runs basically from every single collection of those commitment shards to the actual like raw core shards and all the way through. So I'm going to, again, not draw all of this. I'm going to just draw enough that the point comes across. And so when you think about how the network needs to maintain consensus, it's actually only needing to maintain consensus around this little pocket of data here. All the individual core shards will have their own committees that are basically keeping consensus around the broader core shards. And then from there, there's basically like this additional clustering out of something kind of like the broader network itself. Kind of arranges itself into a four layer tree. And so that's basically how data on the network gets partitioned out. But the nice thing about this is that you end up with a very compact flow. This compact flow enables you to prove any piece of data on the network. And so I feel like I might be getting a little bit off course for people who are trying to figure out how this maps to here. So let me talk about that data flow from a more data forward level. So how data ends up on the network is through a series of application calls. The way queue works is that the application itself is declared at the graph level. When you send a message to the network, you are actually declaring like which application you're going to send it to and which shards that it relates to. And if there's any sort of locking conditions that need to happen in order to transform data. But the idea is that an application is effectively picked out of like three different spots on this. So like let's illustrate through the use of what's called a bloom filter because that's basically what this is. So a bloom filter takes three hashes. We'll take hash one of a message, hash two of a message and hash three of a message. And all of these three hashing functions are something different. And idea is that you will then perform a modulo arithmetic interaction on it such that you get modulo 256. And why 256? Because we have 256 shards at the global level. And so from that you have now picked, and these hash functions do need to be different in some way. I don't know if I said that already. But you'll get basically A, B and C. And those three different values indicate which bit you are selecting on the overall shard. Hey, hey, it's nice to see you. Also, it's always interesting to see the names that people have when they're on different applications. Cool, so basically we pick those and then those all will actually share the same data. That it's basically a redundancy of three different components. And then from there, that flows into the individual core shards into the individual or the committees of the core shards and then the individual core shards themselves. And ultimately you get your data settlement down at that sort of composite tree. How does that relate to actually proving out the data? So say that you want to prove that that data is what it says it is. Like you're performing some interaction somewhere else. Like say that, for example, I want to demonstrate that.

you know, I need to perform some interaction that says I'm a member of this server. Like I have the right to perform whatever this interaction is because I'm a member of this server. You would basically, instead of having to come up with some crazy, like in the traditional Web3 context, you would issue a Merkle proof. In the Web2 context, you would have some sort of consent flow, whether it's through that Auth0 service or authentication service you hand-rolled or maybe even some sort of weird token passing mechanism or you have it like just closely wired inside of the API itself so that it knows to make that check. However you want to do it, you would have, you know, some sort of flow that allows info to transfer from one to the other. But there's actually a better way and you can do it from the queue level pretty easy. So all of these things are what are called commitments. And so we use a commitment scheme called KZG. KZG basically lets you do this really neat property that transforms vectors of data. Or if you're more of a developer than a mathematician, it would be an array of data. It lets you take these vectors of data and you will pack it into what is called a polynomial. And I'm sure most of you have been through enough, you know, math in high school to know what a polynomial is. It's a pretty simple concept. You literally have this polynomial that is some sort of value of like AX to the fourth plus BX to the third plus CX to the second or squared plus DX plus E. So yeah, that's your basic polynomial structure. What varies amongst polynomials for single variable, like in other words, there's just an X, there's not a, you know, P of X, Y, Z, or it's not multivariate, just P of X. I'm not going to go too mathy tonight. You have basically what are constants and you have what are your variables and the powers that they're raised to. And so KZG, the paper for the original commitment scheme basically took notice that, you know, these mathematical operations, these constants might be something that are publicly known, might not be, but whatever the case, you can basically take a commitment to it or some unforgably valid attestation to it that lets you later on prove something about that data. And so where KZG is really cool is that you can actually make a very simple vector commitment scheme from it. And so what does that actually mean? So I'll illustrate with a simple example and then I'll talk about how that scales up. So let's say that this core chart right here, for example, has 65,300, or sorry, 65,536. I think I said 37 earlier, sorry, 36. Individual components. And so that would be a pretty long polynomial, but you would start from here. You would have your first constant in the polynomial commitment scheme. And you would take that section of data, that commitment, and you would slot it in for A. Then this commitment, slot it in for B. And you literally have just a collection of bytes. Like we can talk about the elliptic curve point arithmetic, which I've done many, many times, probably exhausted it now for everybody. So I don't know if that's actually worth venturing into, especially because we're already an hour in. But essentially you take the byte string version of that and you will turn that and interpret that as a scalar factor or a number that you are applying to this string. And so you take the first section, put it for A, second, put it for B, third, C, da, da, da, da, da, all the way down. And then, now what's interesting is we've defined the constants, but we haven't defined the powers of X here. We haven't even defined X. And so here's a neat little thing. And this is where a lot of folks, oh, hey, Phil, this is where a lot of folks came in to Q when they were first learning about it and first participating. In order to have these X values be something useful that you can make a commitment scheme out of, these X values have to be secret. And so what you ultimately do is you construct a ceremony where people will contribute random data. And the way that that process works, I'm not gonna dive deep into the math this time because I've done that several times now, but the way that you basically construct the ceremony is that by everybody's participation, as long as one person is honest, the setup is secure. Like this ceremony is what's called a trusted setup. More specifically, this is the powers of tau. And so you're basically calculating out this squared number, this random component, you know, across 65,536 values. And that's ultimately where that ceremony data came from, was people who are participating in that big global ceremony. We had over 200,000 contributions to that ceremony, which was staggering. And still, like even now, despite the fact that we've had so many nodes running and so many people involved, like even then that still blew my mind. So ultimately what you do with that is you use those public points that you get from the powers of tau setup ceremony. And you can use those as your powers in place of X to the 65,536, or so on, all the way down. I'm not going to say that all. And what's neat about that is when you then evaluate that polynomial, it's very easy. You just add the scalar times those points all the way down the line, and then you end up with that output value, and that's your commitment. And so there's additionally this neat little thing called elliptic curve pairing, and that's why we had the G1 and the G2 as a component of that ceremony. And the reason why it was done in the first place is that you actually can then use the pairing relations of those elliptic curves, such that you can make proofs about that commitment without having to necessarily reveal all of the data about that particular commitment. And what's really valuable about that is that when you talk about things like Merkle trees, for example, where you have like that binary tree structure, when you want to actually provide a Merkle proof, you have to provide at least the initial data that relates to the particular leaf of the Merkle tree, plus all of the hashes that climb up as siblings to the root of the Merkle tree, which means that you have O of log N, or log two N, number of Merkle proofs, like the byte vectors that you have to send along to actually provide a Merkle proof. Whereas for a KZG commitment, to prove a KZG commitment at a particular point, you actually only need to provide the particular data that you're wanting to prove and the corresponding proof point, and that's it. So you actually compress the amount of data that you have to send over the wire to prove that data down to one point, which is really, really cool. And so what's really useful about this is that instead of having a Merkle tree and having to like deeply nest this, because if we were to try to convert a 256 to 65, 536 bipartite graph, nested twice to a Merkle tree, like the corresponding size of that Merkle proof would be disastrously large. Like we would spend the majority of the bandwidth on the network just sending out proofs. And to be honest, for those who were here a long, long time ago, that was actually what we did in the original design, which was the reason why I'm saying that that was not a good choice. And so we adopted the KZG commitment scheme because ultimately the savings gained from the aspect of fears around the security of the setup ceremony is a reasonable trade-off. And it's the same trade-off that Ethereum made. Although I do have some contentions about how Ethereum did that and what the security of that ceremony was, but that's neither here nor there. That's been talked about to death. So structurally, at the end of the day, in order to have all these shared nothing shards, we do have these layers and layers of data settlement into core shards. Those core shards, we designed them out such that they only store up to a gigabyte of data. And the reason for that is pretty simple because we have a 65 by 36 times 64 bytes, which I am, despite the fact that I literally just talked about math for like 30 minutes straight, I am not great at it. That is a 4194304, or roughly four megabytes. And then from there, we have 256 of those that they can settle into. And so collectively you have one gigabyte. And so that's, I don't know why I did it this way. And so that's basically how that structure comes down and how that relates to the actual amount of data that can be stored on the network. And so it's basically our maximum shard size. So I'm not gonna dive into the prover model yet for proving that the data has changed or stayed the same. We'll get to that in a bit, but structurally you're thinking of things in terms of maximally allotting a shard size of one gigabyte. And so when you're thinking about how you plot out the data accordingly under the classical model, you'll have to keep in mind that data consideration in the Q model. And so from DM, same kind of concepts, same kind of flow. I'm not going to repeat everything that I just said. And then finally the file storage. And so from the file storage perspective, like I said, in the classic context, you just use S3. In the Q context, obviously you have this idea of needing to store more than a gig of data. Now we set our limit down to 10 megabytes. So I could just cheat this and say, well, that we don't actually have to worry about that right now. But because I'm sure that there will be people interested in building file hosting services on Q, I'll go ahead and at least think this through out loud in terms of how somebody would think about that. And so basically the idea is that you would have a core pointer to a file. Like you would have this structural point that represents the identity of your file, like some of the metadata about it, and then a collection of the individual core shards that that data actually lives in and the order and sequencing of that data. And then you can also have something really nice like a, you know, KZG commitment scheme that is inlaid in that metadata so that you can prove consistency and integrity across all of those individual components. So basically what you would end up with is this interesting little winding route that specifies like, here's the address of the file, it lands out here, and then you land into a collection of these little points that are all replicated that have the actual block of data. So you'd have your structure of that data. And then that defines like, here are the individual core shards that relate to the file, here's the total file size, here's any interesting metadata attributes about them and so on, and then KZG commitment. Cool. And so finally, then we're at the user repository level. And so one of the things that, you know, obviously for Discord, they are just going to very cheaply replicate that in its entire same flow. One of the things that's interesting about Q and Web3 in general is that there's this notion of portable identities. And so, you know, in Discord, if you get banned from Discord, you're just banned from Discord. And that consequence of being banned from Discord means that you lose everything, like you lose access to everything that you ever had and everything about your identity on that place. Whereas Web3 has this notion of, you know, we settled the data onto a blockchain or you settled this data onto Q's hypergraph. And by consequence, you end up with a structure that you can actually just genericize the concept of identity as its own application. And then multiple applications can then take advantage of it. So one of the things that, you know, might be a winner takes all market to do is to define a standard for identity on Q. And very similarly, like you would follow the same kind of path, like you probably wouldn't even use a gigabyte of data to represent that for the most part. But here's where it gets really neat. Because you have a vector of data that can represent a particular user object, you can actually do some interesting relationships on that vector of data. So for example, say that you have a thing about yourself that you wish to prove to others, like maybe you wanna prove your, maybe you wanna prove your location, or maybe you wanna prove your date of birth. And maybe the issuer of this identity is not actually yourself, it's just somebody that relates to you, and you wanna be able to prove that out in a transparent way without actually having to reveal all of your information, just that one little bit. What you could do is that you're leveraging this vector commitment scheme such that those individual vector components are those individual sections of data. So like your date of birth, for example, would be as its own separate element on that particular proof string. And so when you create that KZG proof, you are actually committing, or sorry, proving on the commitment of that data, that particular vector of that data. So you'd be exposing just your date of birth without your name, or just your date of birth without your address, things like that. Okay. Comment a bit on atomicity and consistency across shards. Should related data live in the same shard or it doesn't matter? So that's actually something that maybe wasn't apparent from the way I described this. The shard scheme actually runs out massively parallel. So like this notion of data being separated out into individual shards, no greater than one gigabyte in size is actually the expectation that you will indeed, like you will shard out every single component of your data as long as it's not intrinsically related. Like I just described with that user identity example, that one object would be individual pieces of data in the total vector that's being committed to. But in the context of like, I have my user data, I have my user profile picture, I have maybe my collection of locations that I've like checked in at some place, all of those things would live in totally different places. So in terms of atomicity and consistency, the atomicity and consistency across shards doesn't quite relate into that context. Like you would not want the related data to live in the same shard because that would just like massively compact the amount of data that you're having to store inside a single core shard. And that's actually something that can slow things down for your application. You want that to be as spread out as possible. Now regarding the atomicity and consistency, the way that things work on Q is that we basically have this and so I appreciate that you brought this up because this does kind of segue into the architectural side of things. So when we talk about the nature of this global proof store, like I said, it is a small cluster of individual components. And so you have this at the global level. Everybody is keeping this replicated and there is a synchronization interval that comes into play with 2.0 that essentially enforces that everyone has agreed and if they disagree, well, then I guess they've been forked but everyone agrees to essentially this level of commitments. And so, sorry, my nose is running. From this commitment scheme, you only have to send 19 kilobytes of data across the board and it's at a periodic interval that's less than the overall clock rate of the network. What that means is that you do run into the scenario where you may actually have data that you have a certain aliveness consideration that you might not. Like for example, if you're doing financial transactions and you're doing those financial transactions between two parties, the individual component of that data, depending on your level of trust and concern, you may actually want to wait till you hit that global synchronization level before you actually take it into consideration where you'd consider it to be finalized. But realistically, you actually can usually settle for at the core shard level, at the core shard committee level, that once it has settled there, then it's sufficiently finalized for most intents and purposes, unless you're moving along like millions of dollars worth of USDC if that's on cue. So to illustrate that, so we have our individual or we have our global core shard commitments, and then we have this cluster of core shard committees. And again, not going to diagram all that out because that would be excessive. Oh, shoot. ScalaDraw sometimes gives me a really hard time when I'm trying to diagram stuff. There. And by the way, if you feel like this presentation is a little bit different than the ones that you've seen in more recent history on like Unlonely or on YouTube, the reason is because a lot of those, I do take an effort to do some rehearsal and cookbook style preparation in advance so that I can speak these things through in a completely coherent way with a little bit more fluidity. The reason why I'm doing this differently this time is explicitly because this is kind of a nostalgic journey, and I'm trying to go back to the roots the way I used to do things in those Code Wolf PAC sessions. So this is more or less just like a reflection on the way I used to do these presos. And so we have our 65,536 individual committee commitments here. And then from there, you have the actual data clusters that relate to those individual committees. So from here, when you roll this 256 degree polynomial, KZG commitment into an individual core shard committees, shard or commitment, you are basically locking that in. And structurally from the nature of the network, these committees are run in a, essentially a, not a pure circular ring, but it is in some degree clock-like. So we have, oh, for the love of God, Excalidraw, please do not do this today. Okay. Okay. So from the global level, a lot of folks have seen the pre-mainnet version of the node software running before and have noticed there's this notion of a master clock. This master clock is useful for two reasons. First reason is that it is something that, you know, gives sequencing to the overall network. Like we have some universally agreed state of time. The second thing is that it actually gives you what is kind of an unforgeably random beacon. And so how that's useful, when we have these periodic intervals of the VDF going along, maybe I should use a different shape here so that it's clear that this is VDF rather than data. There we go. So from the master VDF level, you have this sequence of clock events. Oh, come on. Cool. You have the sequence of clock events that are stepping forward in time. And so from that stepping forward in time, we're able to get basically this random string and very similar to this bloom filter process, we can actually take some degree of modulo. There's a little bit deeper arithmetic that comes into play because of this concept called modulo bias. I'm not gonna deep dive that right now, but something kind of like a modulo, just like smooth over that. It's something like a modulo. So that you randomly pick one of the members of that individual committee. And so when I've been talking about this notion of these prover rings that nodes get in, this is why I talk about it to a certain degree of importance about early provers having priority in prover rings. And that led to this very interesting meme where I'm like flailing wildly. And I don't know, I think I was drinking a bang. It felt like those old school style discord meme images about like mods are asleep or something like that. I don't know. Anyway, these committees are broken down into a collection of eight. So essentially every single prover committee shard, or sorry, prover shard committee, God, I am tripping over my words tonight, is essentially a collection of eight in the first ring, then a collection of eight in the outer ring, and a collection of eight. And that just expands out like an onion. And obviously the closer you are to the core of it, the earlier you were as a prover under the proof of meaningful work consensus algorithm, you get more rewards. That's just the crux of it, is the earlier you are to prove on data, it rewards people to be data greedy more than being core shard greedy. You want to cover as much data as possible and be a bit more cooperative than the inverse model, which would be to try to scope out specific core shards and kick people out or something that would be antagonistic. Although that's hard to do and I can get into that in a little bit. But this idea is that every single interval that is happening for the master clock, you are having these nodes individually producing a verifiable delay function proof over the collection of data that they are covering for the committee. Where is ATK? Oh, sorry, I get confused because like, I try to map people's names to the first name that I ever see about them. And so if I see a name that's like on the telegram, but not on Warpcast or on Warpcast, but not on telegram, depending on how I met them first, I usually get confused by the names. Where is ATK? Let me double check and see if the telegram's got stuff going on. Oh my Lord, okay. Yeah, all right. What's this? Okay, cool. All right, back to the stream. Oh, okay, okay. Well, welcome. All right, so yeah, every single selection it's gonna pick somebody different. But what's interesting is that unlike going like a normal clock where, you know, obviously this moves in a clockwise motion, picks the next one in the sequence and goes on. There's an interesting, you know, conundrum that can happen. If you are monitoring the network, for example, like let's say that I am a disruptive actor and I wanna monitor the network such that I am looking at a node that is covering a particular shard of data and I know that they're in the inner prover ring and I wanna kick them out so that I can, you know, bump them up from the slot, maybe I'm like right here. First off, we separate the prover key from the peer key on purpose so that the linkability between those two factors is detached and the proof submission is also mixed through a mixed net, prevents that association. But here's the trick. If you had a direct sequence of this data where it's happening in linear order, you could actually start to suss that out based on the communication timing and traffic. So instead we leverage this VDF proof, which gives you a degree of admixture into the order of which these nodes are picked. So you end up with this scenario where, for example, this next one here, you can't predict who the next one's going to be. It's going to be somebody else. It might be yourself again. It's going to be essentially completely random. And that's the usefulness of Verify.

randomness beacons. And so that's the kind of construct here is that we ensure that that is essentially the selection mechanism that dictates who's the next prover in a given provering to prove their set of data. All of these provers are individually proving that data all the time, every single time, because they don't know when they're going to be called upon, and they need to answer quickly. If they don't answer quickly, then that will count against them. They'll slowly get scored out to the point where they end up losing their place in the prover ring, and there's a bit more mathematics to that than just simply, like, you missed two of these and you're kicked out kind of thing. Otherwise that, again, could be a potential vector for abuse. So that's basically the structure of how we get to the ordering of the provers and how those provers need to hold out the data. We have a certain degree of replication rate that needs to be maintained as well, which is why there's this notion of a three set of a bloom filter at the global level. So it gets spread out across essentially three different prover committees, and those prover committees have a minimum replication rate of eight. So that means a total of 24 machines out there have to hold onto that data in a unique way. And I could dive into the uniqueness aspect. That's just a fun little bit of trivia from the user perspective. You don't even have to think about the uniqueness consideration. So this idea here is that essentially for most data cases, most things that you would be concerned about, like simple social media data and not necessarily financial data, most cases the moment the prover is called upon, if they issue the signal that their proof contains the updated state, then you have essentially your atomicity. In order to ensure consistency, you would need to check those three individual bloom filter items. When things get disseminated across the network and you're sending that request, that actually structurally happens as a virtue of the request. So when you send a message onto the network via BlossomSub, the full version, you're essentially dictating the bloom filter on which you're publishing this. And so you will have three different collective nodes that will respond back to you, obviously through the mixed net, but they'll respond back to you with the confirmation that that data has been settled on the appropriate chart. And that's what gives you a greater degree of certainty of the confirmation and consistency of the data replication. But like I said, if it's something really critically important that you do need to worry about happening at the global consensus level so that you can feel rest assured that the data has been truly settled unforgeably, because you can easily provide a proof related to that data string, that would actually be at the global prover interval. We're still trying to calibrate what the best global prover interval is. Right now, it's set to roughly 60 seconds. That's proven to be a little bit of a hassle. We've seen this, you know, in the early days when we were actually trying to do the full mainnet launch, like day one go, for at least the network consensus part, that ended up not working so well. And that ultimately came down to the fact that we had just an absolute smattering of nodes day one of the DAWN release. And so we had to walk back a lot and test out a bunch of things. And ultimately, that's where we got to today. So realistically, we might actually bring it back up to the full, like, 60 minute level that was originally outlined in the white paper. It was a conservative bet that I took at the beginning, and I thought I could reduce it backwards a little bit. But ultimately, I think I'm going to have to reconvene on the original thought, which was that for financialized applications that really need to worry about the significant quantity that's been moved, then you would basically put your settlement down to an hour, which could suck. But the nice thing is that for most applications, you would actually worry about the settlement in terms of, you know, just mere milliseconds, which is great. So going into the final aspect of outlining this relational mapping that you're talking, that we're talking about here, the final piece of how you would build a decentralized discord in this context. I'm getting very dry throat. Okay. So the last piece that you would want to be concerned with is in terms of basically the actual application layer. So these API components. And so how do you have the programmatic applications that settle data itself? And so application settlement on queue is basically treated the same way that data settlement on queue is. It's just data. It's data in and of itself. And so we have this kind of notion that we've illustrated through the... See what I said about talking and typing at the same time? Can't do it. Oh. Helps if I actually put the right path in. There we go. So we have an example of this here with the token application. And so the way that we construct data on the network, every piece of data, it's something that I didn't quite allude to earlier, but every single piece of data has to follow a schema. It has to have a corresponding schema that defines it. And so that way, on the network's queryable structure of the hypergraph, you have a depiction of the related elements of the data. In classical database terms, you could kind of think of the hypergraph almost as a entity attribute value store. It's not the same thing, but it's something that a lot of people would be familiar enough with that they would at least understand what that's talking about. But at the end of the day, in Q, our relation to the hypergraph is using RDF. And RDF is that semantic web idea that maps out data structures following this particular schema. And so we use the RDF schema to actually represent the structure of the data on the network, and that actually relates to how applications interact with that data on the network, which is where I was going with this. So structurally, we'll have these notions of requests, for example. And so, like, say, for example, in our token application, we have a create transaction request. So we've constructed this create transaction request schema in RDF. It's following a few additional schema components that we've already defined called QCL types. These relate to the actual generated QCL, which is a subset of Golang down below. And so you can see the create transaction request structure that it generates from that given RDF. And then ultimately, those types get used in the actual code that is created here and deployed as a separate application type on the network. And so this basically defines the interaction surface with those particular types on the hypergraph, and ultimately dictates also, if Safari wasn't acting so terribly right now, all the actual interactions and kind of like the locks that happen on the data on the network. Now, we're actually at about an hour and 30 minutes in, and I still want to get to helping people directly on the part about hacking on Q. Like, how do I build Q? How do I build applications on Q? And how do I do all that? And I don't want to leave those folks hanging. So I'm going to move from the application angle real quick. Say, you know, like, that's essentially the equivalent. Yeah, I know, right? I actually have only had one bang energy today, and I think it shows. So we're going to move here from the, you know, application code is kind of like the Docker images for the microservices almost. It's kind of like the same idea. And we're going to actually dive into the process of building Q. So let me hop back over into Streamlabs real quick. And let's see. Here. Okay. Cool. Hi. All right. So now we have Visual Studio Code open, and if you have, I know quite a few of you have already cloned the repo, but this video is going to be reposted to YouTube, assuming I can figure out how to do that. So what I'm going to do is I'm going to walk through the process of actually cloning the repository. If you aren't familiar with the history of why I have an eggplant in the terminal, there is an issue on the Node.js GitHub repo that I'm referencing. I'm happy to post a link later on that. Okay. So we're going to get clone. Basically somebody was talking about how the eggplant emoji being used as a discussion, like a reply in Node.js's code of conduct discussion was somehow like extremely offensive. And so it led to that user getting banned, which was just hilarious. And so I've kept it as my terminal prompt character ever since. Okay. So we have the ceremony client. I'm going to go ahead and pull it up, and we're going to do a walkthrough. Okay. Oh, shoot. I just realized that, see, I can really prove how rusty I am with streaming because I'm still on the terminal. Shoot. Okay. So now we have VS Code pulled up, and structurally the Q repo looks like this. We have a couple of different directories that you'll want to enumerate through. And I'm going to just do a quick code walkthrough for people who need to understand how to build the Q ceremony client or Node software because it's changed a lot in recent revisions. So if you are wanting to build from source and you're wanting to build against Q directly, this will be very important for you. So first things first, we have two Rust packages now, the VDF and the BLS48581 package. And thankfully, we have some simple generate scripts. But first and foremost, you'll want to install Rust. I'm not walking through installing Rust. You'll want to install Golang 1.22. Not walking through that. But you will want to install a few other things. So you'll want to install libgmp. So let me pop over here real quick. I think it's libgmp. Pretty sure. Trying to remember the package. On a Mac, you use brew. On Ubuntu, you use apt. And I'm pretty sure the package on Ubuntu is libgmp-dev. Maybe it's gmp. I truly can't remember. Yeah. Okay. It's gmp. Cool. So once you have gmp installed, you can go into the ceremony client folder. You can go into the VDF folder and run generate.sh. And that will proceed to download all the Rust dependencies and stop hackering. They'll let you download all the Rust dependencies and then also build out the package. One other thing you'll need is unify. I can never remember the command for that. I'm literally going to just copy it. When it's done. Oh. Okay. Never mind. I have unify already installed. Okay. Cool. So I did that. I'll go ahead and do the same thing into BLS. I know that it's, like, really unhealthy for me. I just I have to. Okay. And it's just about done. And so when I was talking about those commitments that were happening, that was BLS48581. When I was talking about the nicotine good, vape bad. I do actually normally use zens, but I ran out. When I was talking about the verifiable delay function, that's the VDF. So those iterative sequences come from the VDF. Okay. So now that we've built that, then we are actually at the point that the scaffolds that are needed to bind to golang exist. And so we can go into the node folder, and we can run build.sh. That being said, there is actually a lot that gets packed into that command. We only have support for macOS and Linux on that build script. So I want to walk through the details of that in case you're on a different platform and you want to build this, that if you're at least somewhat familiar with, you know, build tool chains, yeah, no, I normally do use zens. But anyway, when you're wanting to build this, you'll need to use the corresponding linker flags that relate to your particular setup. And so what I'm going to do is I'm just going to copy that whole thing, and then going to explain it a little better. So here we have go build, which a lot of you will be familiar with. Oh, shoot, I didn't realize I'm missing a few other things. So there's this cgo enabled one. You have to have that set. Go experiment arenas. We're actually going to get rid of that pretty soon, because arenas have been deprecated anyway, and we don't use them anymore. Let's see, is there anything else? Oh, yeah, the binaries directory. And that would be the target release. I'll get to that in a sec. Okay, so I'm going to copy that back, and then we're going to prepend this. So go experiment arenas as cgo enabled one. cgo is Golang's C bindings, so it will actually do linking into like native C code. You have to have that set in order to use these Rust binaries, because these Rust binaries use Unify, and Unify creates the necessary scaffolding such that cgo can pick up on them. It's a long, long tool chain now. And then finally, I'm going to set binaries stream ceremony client target. Oh, darn it. I think it's target release. We'll find out very quickly. And then dot slash dot dot, which means build everything under here. You're off the track. What does that mean? Oh, library VDF not found. Which means that I didn't get the right directory. Target. Release. Huh, they're there. That's weird. That's weird. I missed something. I don't think I missed anything. Oh. You raise. No, no, no. I did build that. Because I maybe I just misspelled it. Streams. Oh, right, right, right, right. Okay. Here's the problem. No, I can't use that. That's the I can't use that. I'm not commenting on that. Not commenting on that. Okay. Cool. It's built. So, now we have where is the actual huh. Okay. I guess I can specify the output. Okay. Now I'll actually if I can spell it right. Okay. Now I'll build it out. It'll output to a binary name node and we'll be able to run it. Okay. So, now I'm going to build it out and I'm going to run it. Okay. So, now I'm going to build it out. It'll output to a binary name node and we'll be able to run it. Cool. Okay. So, this is going to immediately fail because I don't have the signature check enabled or disabled, I mean. And now you can see it runs. Okay. So, now we have built Q and Q is running. From here, I'm going to dive into a few of the details about Q's operations. So, we have these data workers that are separated out now. And this is very much in line with the shared nothing philosophy or the one thread per core philosophy. We unfortunately haven't fully made the master node itself the one thread per core and so we're actually running into some interesting issues around that. Right now it's also generating a self test. That self test is initially calculating how fast the VDF runs for a given parameter and then starts calculating how fast the BLS48581 library runs for a given set of degree polynomial commitments and proofs. And so, you can see here the 65,536 degree commitment takes a lot longer. But for people who have been running this a long time, you'll find out that it runs a lot longer in the past. So, moving this moving BLS48581 from Golang to Rust was a huge, huge win. This is also another reason why I discourage immediately jumping into one gigabyte like data shards because building the commitments and proofs for those take a lot longer. And so, if you're wanting to prove out a lot of data and you're wanting to, you know, make your interactions on the applications faster, you'll actually want to spread out the data more into individual pieces as they're relevant. So, we're going to let this finish running and then true. True what? Or is it just the remark I made? Okay. So, I'll go ahead and let this keep running. It should immediately pop into, oh, right, shoot. I just realized I am running this in main net mode. I didn't mean to do that. Crap. Okay. Let's give it a few more seconds. This is the boring part. I really wish I could do this once a week. I might be able to get back into the swing of doing it biweekly. I don't think I'd be able to do it weekly. There's just so much going on these days. What's the problem running it on main net? Because we're going to be doing some stuff that will definitely get our peer score down. That's part of the things that I was going to walk through. Yeah, it's not boring for the CPU. It's getting the work out of a lifetime running Q. I think I can do biweekly. And I think that'll be really helpful for, you know, getting developers acclimated to building on top of Q. But I definitely can't do weekly right now. Maybe in the future. So, anyway, what I meant to do is set network to one. Sorry about that. Well, oh, no to specify. Still using default boot. See, I set settings to protect myself from my own actions. All right. So, we'll go ahead and take a look at the node folder real quick. Inside the node folder, there's a .config. And in the .config, you'll see a collection of bootstrap peers. What we're going to do is we're going to put this basically into test net mode. Test net mode. I need two weeks to do what you did the last time. Thanks. Okay. So, what I'm going to do is I'm going to set up IP4. I think I can get away with localhost. I don't remember how. Maybe there's a trick with libP2P. I forget. I always forget this. And we need to set the P2P value. So, oh, shoot. I just realized I keep switching back and forth from the terminal to the I'm getting to be old. I'm a bit of a boomer. I'm going to go ahead and copy this. This peer ID. And then I'm going to switch over to VS code so you can see what I'm doing. Sorry about that. And then fill out the whole string. So, we have these bootstrap peers. And unfortunately, I'm going to need to do this whole deal one more time. Or not go experiment. Go max procs. And we'll set it to four. Oh, wait. What? Let's try that again. By the way, if anybody knows an efficient way to get Streamlabs to switch between scenes, short of having one of those stream decks, I would be really excited to know those Q combos. I'm not going to try them live because somebody might troll me into doing something that will break it. I'm not dumb enough to do command Q to quit things. But I definitely need to know which ways to switch through these views. Anyway. Okay. So, we're going to run through this again. And while that's running, I'm going to go ahead and make a few quick moves here. So, in VS code, note this is still running in the background. I'm not going to necessarily show it right away because I'm going to be doing a lot of stuff in VS code. So, in VS code, and don't mind this peer prive key. You can steal it. I don't care. This is a testnet key that was just generated. So, it does not matter. Okay. Once it's done with this, I really wish I could speed up that commitment scheme a little bit more. I think there's a parallelization trick. Let me actually walk through what the hell it's actually doing behind the scenes. So, in BLS48581, we have commit raw, prove raw, verify raw. In the actual Rust implementation, we are in the Rust implementation, we are running a essentially a fast Fourier transform or a discrete Fourier transform to be more specific. And we're doing it over the domain of the elliptic curve. So, if you're not familiar with that, don't bother messing with this code. But I do believe that the recursive work we're doing here, like when we step into the recurse FFTG1, I think there actually is a trick we can do that can speed up these recursions. Because we do split it. It does split in half. It's a divide and conquer approach. So, it should be embarrassingly parallel. It might be something we can do. I just haven't explored that. If somebody wants to tackle this and knows how to do, you know, FFTs in over, you know, BLS48581 or elliptic curves in general, it would definitely be something that people could take on and it would be really valuable if we could speed that up.

Okay, still generating the final proof metric. Yeah, easy. Okay, awesome. And once it's done with that, then we'll be able to make a bunch of copies. Cool, okay, it is now done. All right, I've stopped the node again, and now we're going to copy the config a whole bunch of times. So copy, copy, oops, that did not do what I wanted it to. Come on, copy. Okay, now we're just going to rename them. Config two, config three, and config four. And while I'm at it, I'm gonna eat that. Eat that, and eat that. Okay, great. So now we have, oh, also eat the keys. Not that they actually matter at this exact point in time, but we'll go ahead and eat them. Okay, yeah, we're gonna do four different node instances in a local test net, so you can see how to prop one of those up. So we're going to go ahead and do config, config two. Oh, yeah, it regenerated, great. So now we're going to take that, and we're gonna, oops, we're gonna specify this, and we're gonna repeat this process. I'm also going to show what I'm doing on the terminal so you see what I'm flipping back and forth on. We'll repeat this process a few more times so that we have those config files generated, and then I will pop back over into VS Code, and we will go ahead and copy those over. Now, we only have generated the pure ID for one of those. Technically, we already have them. The pure private key has been created, but I'm gonna go ahead and pop back into here and list it out, so that way I can see what the hell I'm looking at, because the other thing that we need to change is we need to change the port. So this listenMultiAdder needs to change to TCP, and this one needs to change to, this is like watching Oppenheimer build the atomic bomb. Not quite, not quite. Okay, so now we have that, and then we'll do TCP for that one as well. Actually, 834, oh. And then finally, TCP 8342. And in case you're wondering why I'm doing TCP instead of UDP, there's a little quirk with libP2P. Sorry, UDP with QUIC, for some reason, the implementation that libP2P is using, it just fails. It does not work. If you want to use libP2P in a local testnet, you have to use TCP. I don't know why. I, it's one of those things that I haven't bothered to figure out, because if I further deviate, oh, I just realized, goddammit, I didn't switch back. Okay, sorry. So I switched all these over to have different port numbers, and I'm an ass because I didn't actually show that originally, sorry. I'm gonna copy the pure ID over for pure four, and this time I'll leave it on VS Code because, frankly, it's going to be me just running the same command on the command line that you've already seen. So let me set this back. What the heck happened? Oh, that's what happened. Okay. IP four, 9712701, TCP 8342, P2P, and then the pure ID. Okay, the other thing I'm going to do real quick is I just realized I have it set to all aughts. These need to be, this needs to be local host, and I believe the default on Mac will make it actually choose the default external adapter instead of local host. I do not need this communicating to the outside world. Okay, so now I go back to that other one, we're gonna drop it into these so that we have a bunch of those set up. If somebody wants to make a script for this, that would be really cool because it is kind of a pain to set this all up. You only have to do it once, but it is something that I find myself doing on occasion, and it would be cool to not have to. Okay, so now I need these two pure IDs. So I'm going to, I'm gonna pop back over to the terminal, and we're going to run config two for pure ID, config three for pure ID, and now we have the pure IDs for those. Did I actually copy the wrong one? Yeah, I did. Okay, great. Okay. So copy, paste, and grab the other one, copy, paste. Cool, okay. So now we have our pure list. Cool. Okay, and let me swap back over to Visual Studio, or sorry, to the terminal from Visual Studio, and we will give this a whirl, and I'll pop open four terminals so everyone can see this. Let me make sure I got this. Yeah, okay, it's working, cool. Okay, so we'll start off with config one, go max prox. I'm gonna grab that command that I just ran. Oops, helps if I actually grab the whole command. Great, okay. And then, oh, right, there's one other little gotcha. I'll get to that in just a second. You'll see it happen. So I'm gonna let this run. It runs super fast. It started up, begins streaming from Bitmask. It's trying to connect to those peers. You can see that the peer store increments. You'll also see that because I initially started this in mainnet mode, that those, that peer store count is 35 because it stored all those mainnet peers. That's actually bad. So one other thing. The first time you run this, don't run it with, don't run it with mainnet. You'll make it try to connect to the outside world even when you didn't intend to. So I didn't switch on the stream, but I just went back to Visual Studio Code and deleted those store files. Okay, cool. Now it'll rerun. Now it'll regenerate the store. And this time, the peer list is not going to be the massive list of all the bootstrap peers. Yeah, yeah, yeah. Testnet is just one, but in actuality, any testnet or any number that you use for network is, besides zero, is just a testnet. If you want to run multiple testnets, that's your way to do it. You can think of it kind of the same way that Ethereum has this chain ID. Yeah, mainnet equals zero, yeah. Zero index all the way. Okay, so now that we've launched this, we're going to launch the second one. And you can see it is completely falling apart. And the reason is because of the thing that I was saying. I just realized when it launched that I didn't specify the data workers. So by consequence, it is trying to use the exact same ports which are already in use over here. So we can't do that. So we will need to actually define the data workers. And to do that, we are going to need, let me see if I can grab this real quick. Okay, cool. Cool. By the way, are y'all able to see this Visual Studio Code instance well? Do I need to up the size of this? I think I should, just in case. Okay, so over on the engine, we'll want to add data worker multi-adders. And then we'll specify those out. So for the default config, we actually don't need to do that. It handles that pretty gracefully. But for these, we will. So I'm gonna pop over here because I cannot keep track of incrementing numbers on different screens to save my life. So we will set this to, see, it creates three workers for a parallelism of four. So that means that it will be, actually not script, I'm just gonna look at it, is port 4,001, oh, so 40,001, 40,002, great. Okay, so three, four, five, cool. Okay, so now we have our data workers and then we'll go ahead and drop in those on here and increment those same way, six, seven, eight. Great, go over to four. Nine, 10, 11. This is nice to see how I get resolve each steps the way I think. Oh, yeah, that was one of the things that I used to do in the streams is I would think out loud pretty heavily. So that way people could see why I'm doing what I'm doing instead of potentially falling into this trap of seeing somebody blaze through Vim commands or something and just being completely blind to it. It's actually the reason I adopted VS Code in the first place. Because back in the day, I did used to just use Vim exclusively. I didn't bother using an IDE. And unfortunately, when you're in the process of teaching others how to code, or you're doing, like at Coinbase, for example, we did a lot of pair programming. When you're doing that, you actually unfortunately create a problem because if they aren't familiar with Vim or they can't see what keys you're typing because you're typing too fast, then the learning is just not there. So I talk out loud a lot in these sessions to make sure that it really gels what I'm doing on the screen. Anyway, so now we have that in place. We can do this. Why? Why did you fail? Don't fail on me. That's not acceptable. Okay, we have the data worker multi-adders. I saved it. What? What in tarnation? Data worker multi-adders. I put that in place. Am I crazy? Probably. Okay, let's try this again. Nope, okay. Still not working. Why? Okay, data worker multi-adders is in place. Is it in the right section? Am I just having a senior moment here? That's part of these days. Sometimes I just feel like I'm so old that I'm starting to have like those Alzheimer's moments. Alzheimer's moments. Okay, so engine config is structured to have data worker multi-adders as a string and in config two engine data worker multi-adders. It's right there. Oh, it got defined twice. Okay, that's on me. I forget. I've been operating off of a bunch of old configs that I generated a long time ago. I forget that when it creates a brand new default one that it adds defaults on all those. That's on me. That's on me. Okay, cool. Okay, now that we have those in place, let's go ahead back to the terminal and this time we will run it and it will actually work. Ta-da! Okay, problem is we actually now need that to be present. So I'm going to do something a little funky here. Helps if I have the right thing copied. This is going to fail. I'm actually going to stop it now. Okay, so I'm going to grab this, head over here, core one ampersand, core two ampersand, core three ampersand. Okay, so now all three of those are running and then I can go back over here and start this back up and this time it won't fail out. Cool. Second time here, we're going to do config three, core one, oh shoot. Core one, core two, core three. And then finally, we're going to repeat this one last time for config four. Normally you'd make a script to do this for yourself so that you don't have to actually manually do this but I'm trying to show all the various steps that people would do here. Okay, so ran three and then finally I will run four. All right, that's our entire cluster. All of the data workers are spawned. The one instance has been running for 29 increments on test net and these should start coming up. Now we're almost at the two hour mark. I am not going to stop yet because I feel like there's still lots of cover. Oh, hey Nebu, there's still lots of cover and so I do want to actually go through making some changes, experimenting with the messaging model, understanding the messaging model a little bit better and talk through some of these scaffolds that I put in place for some of the intrinsics. Okay. Yeah, sorry to the folks on the East Coast and that was the current time in England. Yeah, it's 426 in England. That's rough, that's rough. Yeah, unfortunately the downside is like, I work in so many time zones that I just, it's hard for me to make a time that works for everyone. Back in my days at Coinbase, I worked with a team that was based out of Israel and that's actually the reason why I know Hebrew for people who have been wondering why I speak Hebrew and obviously that time zone's way off from US time so I've had to like bifurcate my schedule to like Pacific time and Israel time and that was fun. Okay, so this one's running. This one is running and this one's running. Okay, great. Our network peer count shows three, which means each node is connected to all the other three peers, which means everything is working as expected. Did I go to Hebrew school? No, I actually, I did have a private instructor at first and then I just branched out into learning by immersion, joining certain communities that there were just like language learning communities and a lot of Duolingo to brush up on vocab. It's the same thing that I do for pretty much every language I learn. It's just the process and how I do it. Okay, so cool. All right, now everything's running. So I'm gonna go ahead and actually we can verify this. So this just, oh shoot, no, I can't. I was gonna say we can verify this by calling the RPCs but I just realized I can't do that. Can't do that yet because I didn't expose them. Swear. Okay, so back to VS Code real quick. Gonna edit the config. This is gonna be valuable because this is how you can know that the local testnet is running and is healthy. So we're gonna go ahead and enable that. Again, local host, TCP, and we'll do 8337. You know what, actually I'm gonna spare myself some of the grief here and I'm going to enable this. Now, if you paid attention just a few moments ago, you'd know that 8338 is taken. So I'm going to do A3D5. Okay, back to the terminal. Cool, all right, now that's running again, we'll wait for it to pick up on some of those broadcast messages. And then once it has, we'll have the ability to query its RPC. So when these nodes first fire off, what did you say to describe their phase? Is it establishing and proving the core? No, when I was firing these nodes off, or when these nodes first fire off, the thing that you're first doing is proving out their own system architecture. Like, you know, what are the characteristics of this particular node? How many cores does it have allocated? How much storage does it have? And how fast is the CPU? And then we prove it. We prove it with the verifiable delay function and those BLS48581 commitments and proofs. Once we've done that, then it actually launches the main node process, which is the master node process, the master clock, and then all of the subsequent data workers. Those data workers essentially let us do all the individual core shard proofs. No, man, it's not obvious. Unless you've been developing this for a long time, I get how a lot of people get lost looking at the logs. These logs, they've been kind of a thorn in my side. They used to be very, very, very... Yeah, that's the thing, that when you first run the node, it does do a genesis ceremony. That's the thing. Let me actually show you that real quick so you know where the heck that's coming from. So when you run the node for the first time, after it does the tests and it launches the master worker, let's go to... Let's go to consensus, master, and then master clock consensus engine. So this is where the... Well, that's beautiful. Oh, right, right, right. Okay, so normally when I develop this, by the way, I split these individual packages out into separate VS Code windows, but because I have such a struggle as a very old person on the internet now, because I have a struggle switching the streams to the individual windows that are running, I just open it all up as one. So you're gonna see a lot of red. Sorry about that. Anyway, when it first runs, it sets up a few basic things, like getProvingKey will actually check if the proving key is there. If it's not, it creates it. When it starts, it checks a few details. Like for example, the master time real gets started. So master clock starts the master time real, which is the consensus path for the master frames. Going to the master time real, when it runs and starts, it checks to see, is there already a frame? Frame zero in the case of when it first starts. Is there a frame? If it returns an error and the error is not found, it'll panic because it should return error not found if there's an error. Otherwise something else is really wrong. The next thing it'll do is it will actually check specifically for the genesis frame, which is what you were talking about. And if that's not found, then it will proceed to rebuild the genesis frame. And so it'll do a reset because it detected that something was corrupted and then proceed to rebuild. So under master time real, there's a function called createGenesisFrame that does exactly what you're talking about. And it has some checks for some of the defaults that have occurred over time. And that's basically the gist of it, is it creates that genesis frame and then proceeds to launch into the normal mode. There is synchronization that can happen between peers. I'm not gonna dive into that right now. Okay, so let me flip back to terminal. Okay, so we have, yeah, all of that's in place. Let me open up another tab. I'm going to forget because it is, oh, it is a day that I've only consumed one bang. And so I'm basically running on fumes. Let me find the RPC for the node RPC and network info, right? Okay. Where is the terminal button? Okay. Disappointing. Okay. All right, so I'm going to curl. Really thought I had a GRP curl for this. Man, let me double check that. I have to be careful about some of the things that I do on stream that shows because I don't want to, wow, I really did not ever call that. Well, that's lame. I don't want to show some of the things that aren't released yet. And so I want to make sure that I don't pull something from history that would reveal something too soon. So let me...

No, I'm not doing it to check frames, I'm doing it to get network info. I have it pulled up in front of me, I'm just constructing it from scratch. Okay, I'll go ahead and just paste that out so people can see what I'm working with now. And then we have node service, get network info, and I'm not in the right folder, of course. Protobus, cool, now I'm in the right folder, great. What do you mean it doesn't expose the node service? Yes it does. That's a lie. Oh, right, yeah, that's right, it's got a namespace to it. That's what happens when I'm tired. There we go. That's a long walk for a short drink of water. So, yeah, now we can see that the other peers are, in fact, showing up in the network info. And very similarly, we want to confirm that they are also showing up in the peer info. And we do, in fact, see those. And so, the one that we're missing, it seems that we're missing 8338, I bet that it's just because this is running so gosh darn slow. What journey are we taking today? We have been, I don't know if you just joined or not, but we have been running through the long history of the dev efforts that I did back in the code wolf pack days up to what that has led to today with the equilibrium network. Okay. It's broadcasting the self-test info, hopefully it pulls up. Looks like it didn't. Okay. We'll have to figure out why that's the case. But that's best saved for a different time. We at least see that messages are indeed passing along. So go ahead and stop those. And then we will pop back over to Visual Studio Code. Okay. So right now we are using gossip sub to do our message dissemination. That's going to change. But what's not going to change is that what's I'll answer that in a second, sorry. What's not going to change is the interface in which you communicate through Blossom sub itself. And so the interface for PubSub, which are these items here, publish to Bitmask, publish, subscribe, unsubscribe, all those, and the corresponding implementation here, the interface won't change. The implementation will change, but the interface won't. Non-technical question. When do you expect Q to be reasonably feature complete based on the full roadmap? That's an interesting question, because feature complete, meaning what specifically? If you're talking about the Equinox and Event Horizon releases and all those things, that's a different kind of feature complete than 2.0 main net. 2.0 main net, we already have a set date for, or one of two set dates, but it's looking to be the latter set date. So when you're talking about, oh, no longer an active development, I've achieved what I've set to do. That's a tough question, because the way that Q works is right now it has a limited series of intrinsics that it supports. So in order to facilitate certain types of applications, let me illustrate. For example, I wanted to support machine learning on the network. Machine learning requires a certain arithmetic operation called matrix multiplication. That's one of the primary things that is necessary in order to actively execute on a network. You also have a series of activation functions. Those activation functions are a blend of different things. Some of those work really well if you're doing raw floating point instructions, but remember, Q is an MPC-based network. Q uses MPC in order to calculate these things, and floating point over MPC is kind of a cluster. And so in order to adequately represent an activation function over a MPC-based network, we could encode the entire, I forget the IE number for it, but we could encode the entire floating point circuit. That would be really dumb. There are actually efficient MPC methods for basically doing the same type of activation functions that a normal, trusted ML environment would do. Q doesn't support those right now. And so that would be one of the intrinsics that you would want to add, and you would have to add it in the actual node project itself. And so when you say, like, when do you expect it to be feature complete in the sense that it's no longer an active development, I've achieved what I set to do, I don't think there's an answer to that. I think that it's going to be something that will continue to evolve over time. Same way that Ethereum has, same way that Solana has. I mean, Solana just now added ZK compression, and it's obviously limited on the things that it can currently do, but this isn't something that, you know, it's not something that will ever truly be done. There's always going to be things that come up. Another great example that is in the long term, and it's actually one of the reasons I did what I did with some of the design decisions today. So Q has certain keys that were, you know, certain keys like ED448. It has BLS48581. And this was one question that occasionally I'll get, which is, like, why use those larger keys? Why not use ED25519? Why not use BLS12381? That way you can take advantage of all the other tooling and ecosystem that already exists on that. The reason why I didn't do that is because in the event that elliptic curve cryptography is ever broken, whether it's, you know, pairing-based cryptography that's broken based on some optimized attack, or regular elliptic curve cryptography that's based on some optimized attack, obviously quantum compute changes that a little bit. So, you know, quantum computers using, what is it, Grover's algorithm for discrete logarithm? Or is it Shor's? I can never remember. I always forget. One of them handles integer factorization better, one of them handles discrete logarithms better. Either way. Whichever one a quantum computer is capable of cracking, or in the event that there is actually a polynomial time-based attack, or at least something that reduces the amount of time necessary to attack an elliptic curve in the classical computer context, either of those happens, it's going to happen to the smaller keys first. And so this gives us time, like, all the attacks that are being conducted today are being conducted on the 256-bit curves, like 25519, or Secp256k1 in the Bitcoin and Ethereum case. And so if such an attack is melted, and we learn about it, they're going to suffer first. The parameters necessary to attack ED448 and BLS48581 are exponentially larger. So even under a construct that is successful at attacking the lower-bit strength curves, we still have more time. And so that's kind of the vision that I have with all of these things, is that not everything is going to be the most efficient right now. Not everything is going to be the best right now. Not everything is going to be the fastest or the most optimal. What we're doing is setting the foundation so that, no, no, no, that's the thing, this isn't quantum-resistant. BLS48581 is not quantum-resistant, ED448 is not quantum-resistant. What it is, is it's more complex. And so the sooner that there's a quantum computer capable of attacking ED25519, we still have a lot of time before 448 is cracked by that same computer, generally speaking, unless there's like this absurd exponential speedup in quantum computers, in which case then everyone's threatened and nobody's okay. During that time, we do get the advantage that we are able to actively start building out quantum-resistant things. So one example of this is that the way that we do MPC execution on the network, we use simple oblivious transfer-based MPC, ferret is our OT primitive that we use, which is capable of performing things like running AES circuits over an oblivious transfer runtime in a sufficiently fast amount of time that you can actually livestream decrypt data in an MPC context in a usable speed, which is just, you know, that doesn't exist for things like fully homomorphic encryption. But because we already have those underpinnings of running MPC in this context, and because we use MPC-in-the-head, which is a specific type of ZK approach, in the offline context, we actually can translate that over into MPC-in-the-head-based ZK signature constructions. So say that ED448 falls apart, say that BLS48581 falls apart, we'll have to revisit 48581. If we get that kind of speedup, we can actually benefit by using Merkle trees. That'd be fine to switch to. That wouldn't cause us a problem anymore at that scale of compute, so that's okay. But in the event that there is a problem with, you know, ED448, because we already have MPC-in-the-head as our offline prover construction, we can actually use it for a signature scheme that is known to be quantum resistant that uses MPC-in-the-head. One example of this is PICNIC. And so because we already have those primitives, because we're already proving it out, that sets the roadmap so that we have an easy upgrade solution moving forward. And that's the type of long-term thinking that is based on, or long-term thinking that this is all based on. And so one other thing to think about, when I was talking about machine learning and I was talking about those activation functions, do we have intrinsics for that today? No. When mainnet launches in 2.0, do we have the intrinsics necessary to do at least the matrix arithmetic? Yes, because the way that we do our mixed net uses that same matrix primitive. And so things like that, those accumulative, it's almost like an avalanche. Like you just continue to add intrinsics that are something that broadly impacts the amount of things that the platform can do in positive ways. It all accumulates and has this kind of exponential effect where eventually it's something that overwhelmingly tackles practically every problem that occurs on the web. Something that we need to have a related service for. We're starting small. We're starting with the easy ones. We're starting with basic tokens. We're starting with basic MPC signing. We're starting with basic file hosting and basic key value stores. And those things are the most pragmatic, because they're things that are most used. I'm sorry for rambling on a bit there, but that's to really outline the strategy, the vision, and why I don't really take this answer of, yeah, we're starting with the hello world of today's stream. So to dive into developing on Q, like I was saying, let me make sure I've got the right thing selected. Okay. It's been a minute. Sorry. So diving into that, the first thing that you need to understand about Q is that the nature in which the network communicates is over this Bloom filter-based shard system. And we have this kind of, like, scaffolded over in the form of this Blossom subclass and ERP type and the given interface for it. In order to actually do that communication between multiple parties, we have to have some form of cryptography that enables it to be secure between multiple parties. And there's this class of attacks called man-in-the-middle attacks. And the nature in which you have communication that goes over the network, you have a couple of different solutions for it. You have, like, trust on first use, which is what we use. You also have, like, web of trust, which is something we'd like to roll out on the network later on. And there's a couple of other options. You could have certificate authorities. I don't like those, because I don't like trusted authorities in general. But when you generate those keys, they give you something that you can plug in to... Where is it? Oh, there it is. You can plug in to given communication channels. And so the double ratchet intrinsic has been here for a bit. A lot of people have seen this. And this enables you to actually create a simple double ratchet session that you can communicate between two parties, like, you know, Alice and Bob traditional style. And then now we also have triple ratchet. And triple ratchet is basically an extension of double ratchet into group messaging. So say, for example, I'm building a... I'm building a clone of Discord. And I want to have the communication on those servers, I want those to be end to end encrypted between only the participants of that particular server. What triple ratchet lets you do is it lets you each contribute key material in a way that is efficient, such that you can maintain the security properties of double ratchet, which is perfect forward secrecy, you know, forward secrecy, backward secrecy, and also the repudiability nature of double ratchet, which is something that a lot of projects omit, they don't care about. But that's actually a really important thing in maintaining the overall secrecy of the network and deniability of the network. And triple ratchet reserves all those properties as well. Now, constructing a triple ratchet session is about the same as constructing a double ratchet session, which is why the test case here is basically the same idea. So testing that, you have a collection of keys. You have an identity key, you have an ephemeral key, an assigned prekey. And these are things that if you have ever read signals docs, then you have a good understanding of what those keys do and why they exist. And then you basically construct your initial, what do you call it, basically the message encryption key and the root key based off of what is called a triple Diffie-Hellman exchange. If that's not important to you, then basically it just lets you create a mutually agreeable key. And then once you do that, you have enough material in order to start the participant. These are the two things that you have to do that's a little convoluted. I think I'm going to add a wrapper layer around it so that you don't have to think about it in terms of keys. You can just say, like, identify participant, create session. So you don't have to actually even worry about putting in the right info to make executing this correct. But then from then on out, you don't. You don't have to worry about that at all. It's just ratchet encrypt, ratchet decrypt. And you can see I tend to use Star Wars prequel memes as my test data. You can see the same thing in the KZG tests as well. It's got the Lord Plagueis the Wise copy pasta. But that's basically the gist of it. It creates a simple primitive that lets you, once you establish a session, encrypt and decrypt data. And so this is one of the intrinsics that you would use when you're building applications on Q for messaging approaches, for example. So to illustrate how that plays out, we actually already had and we've had an example here for a while. Yeah, yeah. Yeah, I agree. It would be helpful. Once I started talking about it out loud, I realized that it was going to start to fall on deaf ears. Because unless you've been in the weeds of constructing double ratchet and triple ratchet, or especially double ratchet, since triple ratchet's new, it could get lost. So let's demonstrate why. Let's demonstrate why this matters. So in simple end-to-end encryption, there's a lot of things out there that uses this. One example is Twitter. Your secret chats on Twitter literally just agree to a single key. If that key is ever broken, then, well, you'll see a great example of this here. Let's say that Alice sends something, and this is super important. And Bob says, wow, that really is. And so you now have an entire communication that has occurred all using the same key. And so say that a hacker comes along and somehow steals the key, steals the key from Alice. Once that hacker has done this, they can see everything. And so why double ratchet's important is because it has what's called perfect forward secrecy. So, for example, Alice is sending a message to Bob, and maybe sends a bunch of messages to Bob. And then Bob sends some messages back. Let's say that Eve tries to steal the active keys of that session. Eve does not see any, oh, shoot. Thank you. Ugh. Okay. Getting late here. All right. Sorry about that. Gonna reset this. So what we have here is Alice sending a message, super important. And Bob says, like, wow, this is important. Okay. Because this is a single key, Alice steals it, sees everything in the past. With double ratchet, you have an advantage where you can say a bunch of things, and Bob can reply a bunch of times, and Eve steals the keys, does not see any of the previous messages. But what does or what Eve does see is all the subsequent messages that come from that original sender that Eve stole the keys from. So in this case, it was Bob. So Bob says four. Eve sees four. But the moment that Alice says something new, assuming that Eve does not have constant compromise of Bob's machine, which is common, like smash and grab attacks, that the messages subsequently go back to being encrypted, and Eve can't see them. And so that's the power of double ratchet, and that's why signal was such a really powerful force of an encrypted messenger. What has happened in the state of cryptosystems since then for encrypted messaging schemes is things like MLS. So in the group messaging scenario, like, say you have a group message on signal. In order to have a group message on signal, what it's actually doing behind the scenes is doing every single pair of every user in the group has double ratchet, which is just obscenely, obscenely slow. And it's the reason why signal limits your group size to a thousand, because past that, your phone just cannot keep up. And so because it is, it is exponentially explosive. It's basically like a factorial, not exponential, sorry. If the data was stored encrypted for a period of time, but later the key is stolen, you get so I'll actually get to that in just a second. There's actually a trick in construction of encrypted messaging schemes that you should do that preserves that avoids this problem. I'll get to that in a second, though. So there is messaging schemes out there like MLS. And what MLS does is it basically dedicates a central coordinator for epics and basically arranges all the keys of all the users into a big binary tree where they're doing Diffie-Hellman between each party all the way up the tree, treating the generated Diffie-Hellman agreement as a basically treating it like a number, even though it already is, but treating it like a scalar number instead of the point that it is, and then proceeding to do that all the way up the tree. And that gives you a sort of ratcheting scheme, kind of like double ratchet. But the problem is, is that that gives you no, no, no forward secrecy. So say that I'm Eve, I stole the conversation, I can continue to see the conversation as it comes along with every subsequent bump, because I have enough information that the future conversation is fully intercepted. And so in order to mitigate that, MLS has this notion of epochs. And so during those, it will then proceed to do a full group-wise reset and reestablish the session. That provides the break and recovery. The problem is, is that those epochs are expensive, and they have to be done with everybody on board. And because of that, you end up with a scenario that you need a centralized coordinator. You can't use this in the asynchronous context. There have been subsequent papers that have tried to make this asynchronous. The problem is, is that doing so requires significantly more bandwidth, significantly more messages, and significantly more time. I've been in the MPC space for a long time. I've done a lot of things with distributed key generation, a lot of different group messaging schemes in reliable broadcast, and ultimately, I came up with something called triple ratchet, which basically uses an asynchronous distributed key gen with a bumping scheme that lets you continually increment the key in the same way that double ratchet does in between each individual user. And it plays out like this. So Alice says hi, one, two, three, oh, okay, Bob says hi, one, two, three, Carlos says hi, one, two, three, and you can see that from the perspective of all these users that are in the group, they can see the whole message history. All of them can see the message history, because when you create these encrypted payloads with triple ratchet, you send out one message that is decryptable by the whole group. Now let's simulate kind of like the hacker. Let's say that somebody's jumped into the conversation, but this time it's not really a hacker. It's just somebody who's actually joined the conversation. The nature of triple ratchet is something that does provide forward secrecy and backward secrecy. So as a consequence, I add a new person, and they see nothing. What's interesting is everybody still has all their past conversations. So if Danny says hello, all those folks can see those old messages. All Aaron can see in this example is just that hello. And they can respond to, and like I said, can't talk and type. It always goes poorly. And then all the other party members can see those messages as well. And you can keep adding personas. It gets a little bit rough with Wasm, since I'm simulating all of the individual users. But you can see how that plays out over and over and over again. And so that same kind of concept that happened up here with a hacker's point of view, if a hacker did a smash and grab, basically what they would see is this. So say that Frankie sends a message. And then everyone can see Frankie's message. And then a hacker steals the latest key set that comes from Frankie in terms of that triple ratchet session. Any subsequent message from Frankie, that hacker would still be able to see. But the moment that anybody else says something, that would basically divide it such that, you know, the session has had break in recovery. Like it is able to heal in the same way that this did. I didn't create the hacker point of view on this, because it was already a complicated interface, but it's the same idea. Okay, so that's intrinsic number one. Using that is, like I said, relatively simple.

I could make the interface a little bit easier, but this is the basic idea. Let's talk about the shuffle. The shuffle interface is what I was talking about for the mixedet and also for secure machine learning. The way that that works, it's definitely not the cleanest interface. Like I said, I need to wrap these in simpler-to-use interfaces, but what we're literally doing is creating an example here for the permutation part of things, where it creates a matrix that is 6 by 6. The reason why it only lets you specify a permutation matrix with one variable is that it has to be perfectly square in order to be a permutation matrix. It will generate those, it will shemere split them, and then do the MPC operations with it, which ultimately involves creating a bunch of beaver triples and then doing a bunch of multiplications and additions, and you inevitably end up, you can see the arithmetic involved with that, you inevitably end up with a combined dot product of all those matrices, and you can confirm that it does actually match the end result. We also did add an example of Elon beaver, which is a way to do a power series of matrices. This is something that is not as valuable for the MixNet, but it will definitely come back when we get into machine learning. And then, let's see, we also added the hypergraph in the 1.4.20 release. So, you can see an in-memory example of the hypergraph as a CRDT, and this is a very helpful tool for trying to relate and understand how data settles on the hypergraph. So, you declare your locations for your individual shards. In the Q context, this is just basically a giant bit string, like the bit string of the Bloom filter. You'll declare new vertexes, new hyperedges. This is basically all of the intrinsics related to the raw hypergraph, and not the intrinsics related to the RDF on the hypergraph. Trying to feed this out in small steps. So, that way, you can understand how the hypergraph works structurally, and then you can layer on top of RDF, and how RDF works structurally on its own, and how you combine it with the hypergraph. And you can see a simple test case. We create a location, and remember, this is in memory, so you do have to have all the locations that you're sharding out to on the hypergraph. But what's really useful here is that you can see how it actually does that, how it shards it out, how the CRDT is kept consistent between all the shards, so that you have things like vertex 1 and location 1, 2, and 2, hyperedge 1 and location 1, and then we add vertex 1 and 2 into that hyperedge, and then we proceed to create a new hyperedge that contains, or sorry, no, no, no, I added 1 and 2 to the hypergraph, not the hyperedge, sorry, my bad. But we do add 1 and 2 to the hyperedge that gets added onto the hypergraph as h2. But what's interesting about that is you can see that the location of h2 is location 2, but it still relates to location 1. So, in order to keep consistency on that as a CRDT, you have to perform a few different interactions and consistency checks that happen along the way, and you can actually play through this and see how those are, those assertions are met. And so you can do a lookup at the vertexes that exist, lookup at the hyperedges that exist, you can do a get-reconciled-vertex set, and you can confirm that from that location on that hypergraph, it only shows that one vertex, and on location 1, it does not, because it's asserting false, does not have v2. Similarly for the vertex 2, in location 2, hyperedges, all those things, and then all the relational operations that relate to removing hyperedges and vertexes, and then they have to follow a specific order for it to actually reconcile that data correctly, or it will throw errors. And that's basically it for interacting with the hypergraph as a true intrinsic. And then finally, we will get into the, where did I put it? Did I forget to publish this? Oh, that would be a real bummer. Yeah, one second, let me actually pull that up. Oh, shoot, did I do it again? I really hope I didn't do it again. I did it again. Oh, my God, I'm so sorry. Okay. Yeah, it's definitely getting late. I meant to show you the actual code here that I was literally just rambling through. There's the in-memory representation of the hypergraph that I was just describing here. Hypergraph in-memory. So sorry about that. I keep forgetting to switch the stream scenes. So you can see the test example of that, and you can see those interactions here. I just realized you asked that question because I wasn't showing it. I was just talking as if I was showing it. Sorry about that. So you can see how that all works and how all those relationships properly reconcile or throw errors as expected here. So sorry. Same thing with what I was talking about with crypto channel double ratchet. Like I was saying, I am so sorry. Here's the test case for all of the double ratchet code, and here's the triple ratchet implementation that you can see. It's a very specific one, to be clear. This is not the fully asynchronous group modification triple ratchet. This is the weak mode synchronous one. There's a reason why this one's here and not the others. And then what I was talking about with shuffle, that matrix primitive, I was literally saying it's right here under shuffle, and the tests around that are here, which is doing all of those matrix multiplications. God. It's been a night. And then here's the Elon Beaver multi-matrix sharing implementation for power series. Okay. Getting back to that, there was one last thing that I wanted to pull up. Oh, schema, duh, right there. Okay. So you can find the RDF parser and you can find the collection of various namespaces that we leverage for the schema repository and QCL. You can see the various things that we added in, basically, things like QCL size, QCL orders, that we have strict ordering on the data types. And this is basically the whole parser code. It's pretty boring. There's probably something wrong with it. It needs to get a little bit more vetting. It's not something that runs on the fly. It's something you run personally. So like the only person you'd be shooting in the foot is yourself if you were to find a parser bug. So I would appreciate it if people do look at it, but it's not something that runs as part of the network. It's something that runs when you're releasing applications. And those are the intrinsics that were added to 1.4.20 so that people could actually play around with them and compose things. How does that play out? How do you actually take advantage of that? And so what you were saying earlier about the data being stored and encrypted for a period of time, but later the key is stolen, I want to get to that one first, then I'll get to the hypergraph one. So for example, say you're building an encrypted messenger. Q has three different types of data permanence. You can store things on the hypergraph, and you can choose to store it for a permanent period of time, like you do want to keep it indefinitely, or you can have it for a set period of time that basically expires. There's also a completely third option, which is taking advantage of the network's overall distribution mechanism for messages around the network blossom sub, which is just ephemeral mode. It doesn't store anything on the network. All it does is it propagates messages. Why that's valuable for an encrypted messenger. Let's say that you have a number of peers that are wanting to all connect to each other and send messages to each other on an encrypted messenger. You can take advantage of Q's message passing scheme by literally constructing a blossom sub peer channel. Where do I have that? It's here somewhere. Yep. Okay. Starting a direct channel listener and setting a gRPC server up inside of it. So basically define a gRPC server the same way you do in Go, just regular gRPC, and this will literally take your peer that you define as a key and specify a purpose string, so you namespace it, and it will start listening on that direct channel, and anybody that wants to connect to you can connect to you using that direct channel. Once you have that, you literally just call each other's gRPCs like a simple gRPC service, but now it's actually peer to peer. So you get the peer to peer notion for free by bootstrapping off of the network. Say that you wanted to build something that uses these direct channels in order to construct a peer to peer encrypted messenger that is purely ephemeral. If you're on the network, you can get the messages. If you're not on the network, you don't get the messages. So exactly like you described, the data is not stored, so the key, if it's stolen, you'd have to actually have those messages that that key relates to. And again, only moving forward from that same user. Once it increments and iterates, you no longer get that option. How does that work? So to start things off, you would have your peer construct a direct channel listener for that particular key and purpose, so name it whatever the heck you want. Then you would have your other peers try to connect on that same direct channel. Maybe you'll have some sort of rendezvous point. That's what Q uses for constructing hidden services on the network. Say that you construct a rendezvous point so everyone can pick up on that and know where to communicate. Once you have that direct channel and you have that GRPC server, you can actually define just a series of protobufs to scope out your application. And you'll note that we actually have such a case of that, that we already used for the ceremony application. So for the ceremony service, we had a get public channel, which is literally a bidirectional stream between two parties, because this was a, it was MPC, like true MPC multiparty, but every single step in phase of the network was just pure peer-to-peer to PC. But what you have is you basically create an RPC that has some sort of name, you construct a stream based on a certain type, recommend PDP channel envelope because we built all the tooling around it, and then inside your channel envelope you have protocol identifier, message header, and message body. And what fills those in? The double ratchet and triple ratchet participants. When you create these, it creates a PDP channel envelope. So now you have an encrypted scheme that you can send messages back and forth over the network to any peers that wish to connect to it. So you've essentially bootstrapped a asynchronous, like completely lightweight peer-to-peer, effectively stateless, what's the right word for this? Ephemeral. There we go. Ephemeral messenger on cue with barely writing any code. You just set up that gRPC service, set up the endpoint for the direct channel listeners, publish that information so that everyone knows how to connect to it, and then you can just shuttle messages along. It doesn't care. Okay. Diving into the hypergraph topic, if you want to understand that better, let me pop open the browser again. There is one other thing that, oh, what is the paper? What is the paper? Can you publish on the hypergraph? Not in 1.4.20. That's going to be 2.0. 2.0 is when you can actually publish on the hypergraph. Yeah. So that's actually one of the points I was going to get to in definitely this needs to go on to the next session for me to dive into the next part of, like, actually bootstrapping the Howler app that I built here. Bootstrapping that on cue. That will be the next session. But what I was actually going to talk about in that session was doing exactly what you're describing. In other words, you will have the ability to do the ephemeral channels for publishing, but at the same time, if you wanted, you could have an optional message publication that goes on to the hypergraph for a set period of time that says, hey, if you want, you can pick these messages up and decrypt them. Okay. Sorry. Getting off track. I can never remember the name of the paper, but in the QY paper, we have the RDF to hypergraph section. I thought it was very funny that somebody took a look at this and said that it was completely incomprehensible, especially because actually this wasn't the section. It was an earlier section that had that. But even this one, I find it funny that people say it's incomprehensible. This is literally set notation. If you have had, like, college level, you know, what do they call it? Predicate logic. You definitely would already know this basic set notation. So I thought that was kind of funny. But this is literally doing set notation, leveraging the some of the literals from or not literals, some of the keywords from RDF to define those sets. So it does require having a little bit of back knowledge about RDF once you get into some of those aspects, but the rest of this is just straight up simple set notation. And then a little bit of set notation as it relates to hypergraphs, treating the RDFs as triples. Anyway, I'm gonna go ahead and find that paper. What? Okay, that didn't work the way I hoped. Let's go back. Oh, there it is. Hypergraph-based query optimization. This is one of the things I do that I love doing about, like, diving into papers that get published on occasion, is that sometimes you'll find some absolute gems that come from universities that frequently get overlooked, and it shows in the number of citations that they have. Like, there are six citations on this paper, yet this paper is actually amazing. And so I'm actually gonna, one sec, let me see if I can pop this in. Yes, I can. Awesome. Okay, so now people can find that paper. But anyway, um, this paper is great because it has a, uh, there's no way I can show it, can I? Darn it. Okay, well, follow the link. You will find this great paper that came from some folks in, what university was it? National Institute of Technology in Durgapur. That is, like I said, a lot of universities get overlooked, and what's really sad about that is that oftentimes it's just due to institutional bias and a desire to inflate one's own statistics. So there's sometimes amazing papers like this one that just get completely overlooked. What this did is it actually took hypergraphs and used it as an embedding of RDF graphs and Sparkle, which is a query language for RDF, as execution plans that can be embedded into the hypergraph with a predicate-based index that is managed to be kept up as an index, such that finding the material and retrieving it on the hypergraph is actually really efficient, is actually more efficient than doing entity attribute value-based RDF implementations in, like, a relational database. So this gives you a huge performance boost at being able to relate to RDF data. And the irony is that nobody even read this paper, and it makes me so mad. And so, yeah, we use it for Q to do the RDF hypergraph composition, and it's the reason why we do the query execution the way we do. And while, yeah, it does look a little bit rough to read this through, if you actually sit down and read it, knowing how set notation works, obviously you have to have that prerequisite knowledge, but if you actually go through it and understand the set notation, and then look through the algorithms on how it creates the hypergraph from the RDF triple, or sorry, from the RDF graph, and creates the predicate-based index, it actually becomes really, really clear about what it's trying to do. Yeah, formatting sucks. Sorry about that. Maybe the original paper. It's better formatted. But that's it. That's how query retrieval and settlement of data on the network works and keeps the schema format in order to actually keep data structured on the network, keep the relationships bounded, and then we use the hypergraph as a, like, we use the hypergraph CRDT approach. We do it a little bit differently than the in-memory version. The OT-based version is doing a little bit more clever work with the, words are evading me. It's getting late. Okay. With the adjacency matrix, Jesus. We do a little bit of clever work with the adjacency matrix in order to efficiently execute these via oblivious transfer, but at the end of the day, same concept as working with the in-memory version. So once you've mastered using the in-memory version of the hypergraph, it's literally the same thing, and we keep the interface the same as well. So that's basically it. The rest of the intrinsics are going to start rolling out. I think we're going to land on a 1.4.21 release, because there are some issues that people have highlighted with the way that the master worker is working causing some slowness, and I want to get that resolved. We need to make sure that that actually functions the way it's supposed to, and then we'll be on to 2.0, and the rest of the intrinsics will land with 2.0, and then we're on mainnet. So, went an hour longer than expected. Like I said, I'll go ahead and try to keep up a biweekly pace with this, so two weeks from now, we'll get into the next step of this, which is taking our existing Howler base. And by the way, a while back in the early days of Farcaster, I created a Howlcaster, which basically took this Howler app and wrapped it around Farcaster's old APIs. Let me see if I I think I posted a video. Actually, I know I posted a video recently. Yeah, so I took it one step further and also embedded it with 3D so that you could do, you know, 3D interactions with it, and then I also took it one step further. This was, like, in January of last year, not this year. I took it one step further and also fully embedded it with ReactXR so that you could use any WebXR-compatible headset, like Apple Vision Pro or the Quest, any of the Quests, and you could actually start treating specialized casts that reference these specific 3D formatted objects and, like, actually interact with them like you're looking through portals. But yeah, Howler is something that, you know, next time we'll go ahead and revive it for the sake of demonstrating data settlement. It will not be the complete product. I don't have time to actually make Howler a complete product. Maybe somebody else would like to take up that mantle. But it is the thing that influenced Q in the first place. So you'll see a lot of the design influences show up all the way through Q. Okay. I'm gonna go ahead and leave 10 minutes here for folks, if they have any questions that they want to ask that I can't answer, please. Obviously, you know, there's one topic we can't discuss. But any other questions besides that one topic, happy to answer to the best of my ability. And yeah. Let me make sure I've got this appropriate pulled up. I have a phone pulled up with it, but it doesn't it seems like it's not showing everything. I hope I hope I answered everybody's questions. Is anybody working on Q's adoption in the market? Oh, this treads this treads dangerously. I think I can answer I think I can answer part of this question. I'll put it this way. I would not have the conviction that I have in spending so much money that I've spent on all of the server resources for Q, and all the time that I've spent building it. If there weren't already individuals or businesses, didn't have to be very cautious here. And if there weren't individuals or businesses that I was actively working with, I'll put it that way. I need to get more zens tomorrow. Yeah, thanks for watching. This was a lot longer than I. Yeah, that's that's kind of the thing. It's like, I obviously can't pick it up as a just to just read this off, because this is going to land on the YouTube video and people won't see the question, or the statement, your deployment of howler is going to make a great demonstration use of primitives. You can't see me? You should be able to see me in the corner. I don't have a sorry, I lost my old the one that I was using on on lonely. I lost the old template for that. I don't know what happened with Streamlabs, but it just completely nuked it. Anyway, the deployment of howler is going to make a great demonstration use of primitives, etc. Yeah, I'm not going to make it a fully functioning app right now. I don't have time for that. But it is going to definitely be a good demonstration of how to how to compose those. And we'll actually build some of that out in the in the next session. Oh, dang, thank you for that. I can't repeat that out because it's going to land on YouTube. But thank you for that advice. That's actually super helpful. Okay, so next question was, do you see a future where equilibrium could utilize other crypto competing projects such as our weave? What are my thoughts about the community's reaction to the D? I can't answer that. Can't answer that. Can't answer that question. Sorry. It's one thing I have to I have to avoid. But a future where equilibrium could utilize other crypto competing computing projects such as our weave. Is it possible? Yeah. Equilibrium does not have an opinion towards other networks. Like I don't have any opinion about those networks. Like I've of course taken some pot shots. Like I very often historically made jokes about Solana mainnet beta being down because it's it's a funny meme. But at the end of the day, this ecosystem is too small to have like actual PVP style antics between people building real networks. And so the people who are engaged in deep negativity that are just like trashing everyone else in the space. And I mean, I've seen it personally myself, like people have trashed me personally.

about things that don't even relate to Q and that's just weird because at the end of the day this the space is so small and the amount of researchers that are spending their time in the space is so limited and burning bridges like that is a surefire way to make you an island and an island does not succeed for a protocol you can't you can't just be an alone island out there and being able to work with other networks is something that you know we are designed for it was something that was baked into the origins when I was building out howler actually let me let me switch back to it and show you real quick when I was building howler one of the things we did was building out integrations with aetherium and Solana so that you could have connections related to your crypto assets I didn't dive into that into the diagramming session because just same thing like I didn't have like video content showing up in the in the you know design session either but these types of things are critically important in order to actually increase adoption and availability and we've already reflected that one of the things that's you know basically dog-fooding are primitives is our MPC based bridge our bridge that is bridging Q assets over on to aetherium with wrapped quill is using an MPC based signer that is built on top of Q and so while that's not on the public humane net right now it will migrate to it with 2.0 and that's when it will enable the second half of the bridge being able to bring over assets back onto Q and what that also means is the ability to bring aetherium assets onto Q as well and the benefit there is that you end up in this scenario where you can actually start doing some really interesting things leveraging the nature of Q that enhances other networks so in the case of aetherium for example like let's say I made a decentralized messenger like howler and I made that messenger on Q and I wanted to send somebody an ERC 20 and that ERC 20 you know on aetherium it's a public ledger it's a public blockchain but if I bridge that ERC 20 over to Q, Q is anonymous so if you send that asset over on on quillibrium through you know the the transaction means on Q instead of using the ERC 20 semantics on aetherium you actually create a nice anonymous bridge that lets you send assets to people keeping their privacy preserved and then they can bridge it back onto aetherium if they'd like so yeah interoperability is a really important thing I definitely will make jokes from time to time but very rarely am I actually ever critical towards a network as a whole there are a few examples like I've been very critical towards Richard Hart which isn't even his last name I'm like I'm like me that actually is my last name I've been really critical towards Richard Hart because of the things that he's done I've been really critical in the origin yeah no it's not his actual last name I've been really critical towards base especially near the beginning because of the fact that it is a centralized sequencer it didn't it still doesn't even have fault proofs yet and I know optimism mainnet just rolled out there's but base doesn't have it yet there are lots of things that I am critical about in this space and it relates to things that make the space better like people who are scammers I will gladly like complain about people who are trying to say something is decentralized when it's not I will complain about but when you got people who are acting honestly in the space who are trying to do the right thing and maybe they're taking a different route that I might disagree with well well we'll have it we'll have a discussion about that but at the end of the day like it is too small of an ecosystem to be constantly negative it is just it's such a huge waste of time yeah and that's that's unfortunate things is that it's also been something that has made Q a target of attack there are some people who have definitely taken the approach of saying like you know the founder says this is too complicated and so too complicated to understand which one I don't say that I say actually all the time that this is essentially something you can treat as high school arithmetic you just have to relate to it in that high school arithmetic sense which is why I made the Eli 5 Eli high schooler explainer but yeah like the math is complicated but I don't think anybody is incapable of learning that in fact I actually really love teaching that to people so I really like to encourage people to learn and but yeah I just I don't know some of it some of the stuff that happens in the space makes me sad cool well we have successfully hit three hours of stream time so I'm gonna go ahead and wrap it up for tonight thanks everybody for attending I'm gonna find a way to try to transport this over to YouTube I think there's a way to do that I just it's been a while since I've done it so we'll go ahead and pick up on this pick up back from this on the basically two weeks from now I'll be sure to update the calendar accordingly so yeah thanks folks and we will catch you on the next one